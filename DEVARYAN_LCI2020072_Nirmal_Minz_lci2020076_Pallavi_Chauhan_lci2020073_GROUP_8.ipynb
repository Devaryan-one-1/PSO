{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NUzWwp-K6kjA"
      },
      "outputs": [],
      "source": [
        "#Nirmal Minz : lci2020076\n",
        "import numpy as np\n",
        "import math\n",
        "import random\n",
        "\n",
        "def calculate_flow_interference(task_index, sensor_index, assigned_nodes, sensor_data, communication_data, deadline_data, flow_data, max_time_slot):\n",
        "    communication_delay = 0\n",
        "\n",
        "    for neighbor_index in range(len(sensor_data)):\n",
        "        if neighbor_index != sensor_index and deadline_data[neighbor_index] >= max_time_slot and sensor_data[assigned_nodes[task_index]].count(neighbor_index) > 0:\n",
        "            flow_interference = 0\n",
        "            for flow_index in range(len(flow_data)):\n",
        "                if flow_data[flow_index][0] == task_index and flow_data[flow_index][1] == neighbor_index:\n",
        "                    flow_interference += flow_data[flow_index][2]\n",
        "            if flow_interference + communication_data[sensor_index][neighbor_index] ** 2 != 0:\n",
        "                sinr = sensor_data[assigned_nodes[task_index]][sensor_index] / (flow_interference + communication_data[sensor_index][neighbor_index] ** 2)\n",
        "            else:\n",
        "                sinr = 0  # Or any other appropriate value based on your requirements\n",
        "\n",
        "            # sinr = sensor_data[assigned_nodes[task_index]][sensor_index] / (flow_interference + communication_data[sensor_index][neighbor_index] ** 2)\n",
        "            communication_delay += sinr\n",
        "\n",
        "    return communication_delay\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Pallavi Chauhan : lci2020073\n",
        "def calculate_fitness(particle, task_data, sensor_data, deadline_data, communication_data, reliability_data, flow_data):\n",
        "    assigned_nodes = [-1] * len(task_data)\n",
        "\n",
        "    for task_index, sensor_index1 in enumerate(particle):\n",
        "        sensor_index = int(sensor_index1)\n",
        "        # print(particle, sensor_index)\n",
        "        while assigned_nodes[sensor_index] >= 0:\n",
        "            min_distance = math.inf\n",
        "            min_sensor_index = -1\n",
        "            for neighbor_index, neighbor_reliability in enumerate(sensor_data[sensor_index]):\n",
        "                if assigned_nodes[neighbor_index] < 0:\n",
        "                    distance = (\n",
        "                        neighbor_reliability / (communication_data[sensor_index][neighbor_index] ** 2)\n",
        "                    ) if communication_data[sensor_index][neighbor_index] != 0 else float('inf')\n",
        "                    if distance < min_distance:\n",
        "                        min_distance = distance\n",
        "                        min_sensor_index = neighbor_index\n",
        "            sensor_index = min_sensor_index\n",
        "\n",
        "        particle[task_index] = sensor_index\n",
        "        assigned_nodes[sensor_index] = task_index\n",
        "\n",
        "    total_fitness = 0\n",
        "    for sensor_index in range(len(sensor_data)):\n",
        "        tasks_on_node = [task_index for task_index, node_index in enumerate(assigned_nodes) if node_index == sensor_index]\n",
        "\n",
        "        execution_time = 0\n",
        "        communication_delay = 0\n",
        "\n",
        "        for task_index in tasks_on_node:\n",
        "            task_fragments = task_data[task_index]\n",
        "            task_time_slots = [slot for fragment in task_fragments for slot in fragment]\n",
        "            max_time_slot = max(task_time_slots)\n",
        "            execution_time += max_time_slot\n",
        "\n",
        "            communication_delay += calculate_flow_interference(task_index, sensor_index, assigned_nodes, sensor_data, communication_data, deadline_data, flow_data, max_time_slot)\n",
        "            \n",
        "        node_reliability = max(sensor_data[sensor_index])\n",
        "        node_fitness = node_reliability / (execution_time + communication_delay + deadline_data[sensor_index])\n",
        "        total_fitness += node_fitness\n",
        "\n",
        "    return total_fitness\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Xprnb-s17Sm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Devaryan LCI2020072\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "def initialize_particles(num_particles, max_task_num, max_sensor_num):\n",
        "    particles = []\n",
        "    for _ in range(num_particles):\n",
        "        particle = []\n",
        "        for j in range(max_task_num):\n",
        "            column_values = random.sample(range(max_sensor_num), max_task_num)\n",
        "            particle.append(column_values[j])\n",
        "        particles.append(particle)\n",
        "    return np.array(particles)\n",
        "\n",
        "def initialize_velocities(num_particles, max_task_num, max_sensor_num):\n",
        "    return np.zeros((num_particles, max_task_num), dtype=int)\n",
        "\n",
        "\n",
        "def update_velocities(velocities, particles, best_particles, global_best_particle, inertia_weight, cognitive_weight, social_weight, velocity_max):\n",
        "    cognitive_component = cognitive_weight * np.random.uniform(0, 1, size=particles.shape) * (best_particles - particles)\n",
        "    social_component = social_weight * np.random.uniform(0, 1, size=particles.shape) * (global_best_particle - particles)\n",
        "    new_velocities = np.clip(velocities * inertia_weight + cognitive_component + social_component, -velocity_max, velocity_max)\n",
        "    return new_velocities\n",
        "\n",
        "\n",
        "def update_particles(particles, velocities, max_sensor_num):\n",
        "    new_particles = np.clip(particles + velocities, 0, max_sensor_num - 1)\n",
        "    return new_particles\n",
        "\n",
        "\n",
        "def update_best_particles(particles, particle_fitness, best_particles, best_particle_fitness):\n",
        "    improved_indices = particle_fitness < best_particle_fitness\n",
        "    best_particles[improved_indices] = particles[improved_indices]\n",
        "    best_particle_fitness[improved_indices] = particle_fitness[improved_indices]\n",
        "    return best_particles, best_particle_fitness\n",
        "\n",
        "\n",
        "def update_global_best_particle(best_particles, best_particle_fitness):\n",
        "    global_best_particle_index = np.argmin(best_particle_fitness)\n",
        "    global_best_particle = best_particles[global_best_particle_index]\n",
        "    return global_best_particle_index, global_best_particle\n",
        "\n",
        "\n",
        "def adaptive_discrete_particle_swarm_optimization(num_iterations, num_particles, max_task_num, max_sensor_num, task_data, sensor_data, deadline_data, communication_data, reliability_data, flow_data):\n",
        "    inertia_weight_min = 0.4\n",
        "    inertia_weight_max = 0.9\n",
        "    cognitive_weight_min = 1.5\n",
        "    cognitive_weight_max = 2.5\n",
        "    social_weight_min = 1.5\n",
        "    social_weight_max = 2.5\n",
        "    velocity_max = max_sensor_num - 1\n",
        "\n",
        "    particles = initialize_particles(num_particles, max_task_num, max_sensor_num)\n",
        "    particle_fitness = np.array([calculate_fitness(p, task_data, sensor_data, deadline_data, communication_data, reliability_data, flow_data) for p in particles])\n",
        "\n",
        "    best_particles = np.copy(particles)\n",
        "    best_particle_fitness = np.copy(particle_fitness)\n",
        "    global_best_particle_index, global_best_particle = update_global_best_particle(best_particles, best_particle_fitness)\n",
        "\n",
        "    velocities = initialize_velocities(num_particles, max_task_num, max_sensor_num)\n",
        "\n",
        "    prev_global_best_particle = None  # Track previous global best particle\n",
        "\n",
        "    for iteration in range(num_iterations):\n",
        "        inertia_weight = inertia_weight_max - (inertia_weight_max - inertia_weight_min) * (iteration / num_iterations)\n",
        "        cognitive_weight = cognitive_weight_max - (cognitive_weight_max - cognitive_weight_min) * (iteration / num_iterations)\n",
        "        social_weight = social_weight_max - (social_weight_max - social_weight_min) * (iteration / num_iterations)\n",
        "\n",
        "        velocities = update_velocities(velocities, particles, best_particles, global_best_particle, inertia_weight, cognitive_weight, social_weight, velocity_max)\n",
        "\n",
        "        particles = update_particles(particles, velocities, max_sensor_num)\n",
        "\n",
        "        particle_fitness = np.array([calculate_fitness(p, task_data, sensor_data, deadline_data, communication_data, reliability_data, flow_data) for p in particles])\n",
        "\n",
        "        best_particles, best_particle_fitness = update_best_particles(particles, particle_fitness, best_particles, best_particle_fitness)\n",
        "\n",
        "        global_best_particle_index, global_best_particle = update_global_best_particle(best_particles, best_particle_fitness)\n",
        "\n",
        "        if prev_global_best_particle is not None and np.array_equal(global_best_particle, prev_global_best_particle):\n",
        "            # Stop the loop if the current best particle is the same as the previous best particle\n",
        "            break\n",
        "\n",
        "        prev_global_best_particle = np.copy(global_best_particle)\n",
        "\n",
        "    return best_particles[global_best_particle_index], best_particle_fitness[global_best_particle_index]\n",
        "\n",
        "# Example usage\n",
        "num_iterations = 400\n",
        "num_particles = 40\n",
        "max_task_num = 10\n",
        "max_sensor_num = 10\n",
        "for i in range(20):\n",
        "# Generate random task data\n",
        "\n",
        "  max_fragments = 4\n",
        "  max_time_slots = 5\n",
        "\n",
        "  # Generate random task data with fragments and time slots\n",
        "  task_data = []\n",
        "  timeslots = []\n",
        "  for i in range(max_task_num):\n",
        "      fragments = []\n",
        "      for j in range(max_fragments):\n",
        "          time_slots = [random.uniform(1, 10) for k in range(max_time_slots)]\n",
        "          fragments.append(time_slots)\n",
        "          timeslots.append(time_slots)\n",
        "      task_data.append(fragments)\n",
        "  print(task_data)\n",
        "  sensor_data = [[random.uniform(0, 10) for j in range(max_sensor_num)] for i in range(max_sensor_num)]\n",
        "\n",
        "  deadline_data = [random.randrange(0, 1) for i in range(max_task_num)]\n",
        "\n",
        "\n",
        "  # Generate random communication data\n",
        "  communication_data = [[random.randrange(1, 2) for j in range(max_sensor_num)] for i in range(max_sensor_num)]\n",
        "  print(communication_data)\n",
        "  # print('\\n')\n",
        "\n",
        "  # Generate random reliability data\n",
        "  reliability_data = [[random.uniform(0, 1) for j in range(max_sensor_num)]for i in range(max_sensor_num)]\n",
        "  print(reliability_data)\n",
        "  # print('\\n')\n",
        "\n",
        "  reliable_sensors = [i for i in range(len(reliability_data)) if all(r >= 0.0 for r in reliability_data[i])]\n",
        "  print(reliable_sensors)\n",
        "  # print('\\n')\n",
        "  sensor_data_filtered = [[sensor_data[i][j] for j in reliable_sensors] for i in reliable_sensors]\n",
        "  print(sensor_data_filtered)\n",
        "  # print('\\n')\n",
        "  deadline_data_filtered = [deadline_data[i] for i in reliable_sensors]\n",
        "  print(deadline_data_filtered)\n",
        "  # print('\\n')\n",
        "  communication_data_filtered = [[communication_data[i][j] for j in reliable_sensors] for i in reliable_sensors]\n",
        "  print(communication_data_filtered)\n",
        "  # print('\\n')\n",
        "  reliability_data_filtered = [reliability_data[i] for i in reliable_sensors]\n",
        "  print(reliability_data_filtered)\n",
        "  # print('\\n')\n",
        "\n",
        "  flow_data = [(random.randrange(max_task_num), random.choice(reliable_sensors), random.uniform(0, 10)) for _ in range(max_task_num)] \n",
        "  print('\\n')\n",
        "\n",
        "  result = adaptive_discrete_particle_swarm_optimization(num_iterations, num_particles, max_task_num, max_sensor_num, task_data, sensor_data, deadline_data, communication_data, reliability_data, flow_data)\n",
        "  print(result)"
      ],
      "metadata": {
        "id": "GZ7ErHDI7a7u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}